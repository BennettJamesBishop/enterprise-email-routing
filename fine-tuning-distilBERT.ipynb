{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune DistilBERT For Multi-Class Text Classification Using Tensorflow and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we explore the capabilities of DistilBERT for multi-class text classification by comparing three approaches: no fine-tuning, standard fine-tuning, and LoRA fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents:\n",
    "    1. Data Pre-Processing\n",
    "    2. Metrics We Will Measure\n",
    "    3. Classification with No Fine-Tuning\n",
    "    4. Classification with Standard Fine-Tuning\n",
    "    5. Classification with LoRA Fine-Tuning\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import required packages\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "import tf_keras\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.offline import iplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Split into Train, Validation, Test using Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "root_path = 'data/full_dataset.csv'\n",
    "df = pd.read_csv(root_path)\n",
    "df.head()\n",
    "\n",
    "# Encode the 'category' column into numerical labels\n",
    "df['encoded_text'] = df['category'].astype('category').cat.codes\n",
    "\n",
    "# Separate columns for splitting\n",
    "data_texts = df['request'].to_list()  # 'request' is the text data\n",
    "data_labels = df['encoded_text'].to_list()  # Encoded class labels\n",
    "stratify_values = df['stratify_col'].to_list()  # Stratification column\n",
    "\n",
    "# Split the data into Train/Validation sets with stratification\n",
    "train_texts, val_texts, train_labels, val_labels, train_stratify, val_stratify = train_test_split(\n",
    "    data_texts, data_labels, stratify_values, \n",
    "    test_size=0.2, stratify=stratify_values, random_state=0\n",
    ")\n",
    "\n",
    "# Split the Train set further into Train/Test with stratification\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    train_texts, train_labels, \n",
    "    test_size=0.1, stratify=train_stratify, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. View test/train/validation Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label Mapping (Encoded -> Category):\n",
      "0: Facilities Management\n",
      "1: Finance\n",
      "2: HR\n",
      "3: IT Support\n",
      "4: Marketing\n",
      "\n",
      "Final dataset information:\n",
      "Train set size: 3596\n",
      "Validation set size: 1000\n",
      "Test set size: 400\n",
      "Example train_texts: ['I’m gathering details about rewards for long-term employees and was hoping you could provide some insight. Let me know if you need further specifics from me.', 'Do you have the latest version of the diversity and inclusion policies handbook? I need it for a new hire orientation.', 'Could you share detailed insights on the performance metrics for keyword research for PPC campaigns? I’d like to use this data for our planning.']\n",
      "Example train_labels: [2, 2, 4]\n",
      "Example val_texts: ['Could you outline the steps to optimize our launching retargeting ads approach? Any case studies or examples would be helpful.', 'Need access to server maintenance.', 'Insights on customer retention strategies performance needed.']\n",
      "Example val_labels: [4, 3, 4]\n",
      "Example test_texts: ['I’m experiencing an issue with software installation. Can you assist in diagnosing and resolving the problem as soon as possible?', 'Could you review and approve the budget variance analysis for this month? Let me know if there’s anything I need to address.', 'I wanted to check on the status of my request for access to storage capacity planning. Let me know if further details are needed.']\n",
      "Example test_labels: [3, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "# Map numerical labels back to category names\n",
    "label_mapping = dict(enumerate(df['category'].astype('category').cat.categories))\n",
    "print(\"\\nLabel Mapping (Encoded -> Category):\")\n",
    "\n",
    "for encoded, category in label_mapping.items():\n",
    "    print(f\"{encoded}: {category}\")\n",
    "\n",
    "# Output dataset information\n",
    "print(\"\\nFinal dataset information:\")\n",
    "print(f\"Train set size: {len(train_texts)}\")\n",
    "print(f\"Validation set size: {len(val_texts)}\")\n",
    "print(f\"Test set size: {len(test_texts)}\")\n",
    "\n",
    "print(f\"Example train_texts: {train_texts[:3]}\") \n",
    "print(f\"Example train_labels: {train_labels[:3]}\")\n",
    "print(f\"Example val_texts: {val_texts[:3]}\") \n",
    "print(f\"Example val_labels: {val_labels[:3]}\")\n",
    "print(f\"Example test_texts: {test_texts[:3]}\") \n",
    "print(f\"Example test_labels: {test_labels[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Optional: Save test/train/val to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames for each split\n",
    "train_df = pd.DataFrame({\n",
    "    'request': train_texts,\n",
    "    'label': train_labels\n",
    "})\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    'request': val_texts,\n",
    "    'label': val_labels\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'request': test_texts,\n",
    "    'label': test_labels\n",
    "})\n",
    "\n",
    "# Save DataFrames to CSV files\n",
    "train_df.to_csv(\"data/train.csv\", index=False)\n",
    "val_df.to_csv(\"data/validation.csv\", index=False)\n",
    "test_df.to_csv(\"data/test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Tokenize Data For DistilBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation = True, padding = True  )\n",
    "\n",
    "val_encodings = tokenizer(val_texts, truncation = True, padding = True )\n",
    "\n",
    "test_encodings = tokenizer(test_texts, truncation = True, padding = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    ")).batch(32)\n",
    "\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    ")).batch(32)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    test_labels\n",
    ")).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of our three models, we will collect the following metrics:\n",
    "    1.    accuracy\n",
    "    2.    precision\n",
    "    3.    recall\n",
    "    4.    f1\n",
    "    5.    Training Time\n",
    "    6.    Trainable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Predict Categories Without Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model setup\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=5)\n",
    "opt = tf_keras.optimizers.legacy.Adam(learning_rate=5e-5)\n",
    "loss = tf_keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Raw logits expected\n",
    "model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Evaluate Performance of DistilBERT with no fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 293ms/step\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "1/1 [==============================] - 0s 254ms/step\n",
      "1/1 [==============================] - 0s 242ms/step\n",
      "1/1 [==============================] - 0s 237ms/step\n",
      "1/1 [==============================] - 0s 237ms/step\n",
      "1/1 [==============================] - 0s 236ms/step\n",
      "1/1 [==============================] - 0s 238ms/step\n",
      "1/1 [==============================] - 0s 235ms/step\n",
      "1/1 [==============================] - 0s 238ms/step\n",
      "1/1 [==============================] - 0s 232ms/step\n",
      "1/1 [==============================] - 0s 241ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bennettbishop/enterprise-email-routing/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Training Time (s)</th>\n",
       "      <th>Trainable Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Fine-Tuning</td>\n",
       "      <td>0.2225</td>\n",
       "      <td>0.098889</td>\n",
       "      <td>0.2225</td>\n",
       "      <td>0.084762</td>\n",
       "      <td>0</td>\n",
       "      <td>66957317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model  Accuracy  Precision  Recall  F1 Score  Training Time (s)  \\\n",
       "0  No Fine-Tuning    0.2225   0.098889  0.2225  0.084762                  0   \n",
       "\n",
       "   Trainable Parameters  \n",
       "0              66957317  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform prediction on the test dataset\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Iterate over test dataset to collect true labels and predictions\n",
    "for batch in test_dataset:\n",
    "    input_data, labels = batch\n",
    "    y_true.extend(labels.numpy())  # Collect true labels\n",
    "    logits = model.predict(input_data).logits  # Predict logits\n",
    "    predictions = logits.argmax(axis=-1)  # Convert logits to predicted labels\n",
    "    y_pred.extend(predictions)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "no_finetuning_accuracy, no_finetuning_precision, no_finetuning_recall, no_finetuning_f1 = evaluate_model(y_true, y_pred)\n",
    "no_finetuning_training_time = 0, #There is no training involved, as we are not fine tuning the model\n",
    "\n",
    "# Find number of trainable parameters\n",
    "trainable_params = 0\n",
    "for variable in model.trainable_weights:\n",
    "    # Use the 'count_params' method to get the number of parameters in each variable\n",
    "    param_count = tf.keras.backend.count_params(variable)\n",
    "    trainable_params += param_count\n",
    "no_finetuning_trainable_params = trainable_params\n",
    "\n",
    "model_metrics = [\n",
    "    {\n",
    "        'Model': 'No Fine-Tuning',\n",
    "        'Accuracy': no_finetuning_accuracy,\n",
    "        'Precision': no_finetuning_precision,\n",
    "        'Recall': no_finetuning_recall,\n",
    "        'F1 Score': no_finetuning_f1,\n",
    "        'Training Time (s)': 0,\n",
    "        'Trainable Parameters': trainable_params\n",
    "    },\n",
    "\n",
    "]\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df_metrics = pd.DataFrame(model_metrics)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predict Categories With Standard Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model setup\n",
    "std_ft_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=5)\n",
    "opt = tf_keras.optimizers.legacy.Adam(learning_rate=5e-5)\n",
    "loss = tf_keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Raw logits expected\n",
    "std_ft_model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "113/113 [==============================] - 95s 811ms/step - loss: 0.3717 - accuracy: 0.9116 - val_loss: 0.0139 - val_accuracy: 1.0000\n",
      "Epoch 2/3\n",
      "113/113 [==============================] - 92s 812ms/step - loss: 0.0099 - accuracy: 0.9994 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "113/113 [==============================] - 92s 813ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "std_ft_start_time = time.time()\n",
    "std_ft_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=3,\n",
    "    verbose=1\n",
    ") \n",
    "std_ft_end_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 734ms/step\n",
      "1/1 [==============================] - 0s 286ms/step\n",
      "1/1 [==============================] - 0s 265ms/step\n",
      "1/1 [==============================] - 0s 256ms/step\n",
      "1/1 [==============================] - 0s 266ms/step\n",
      "1/1 [==============================] - 0s 301ms/step\n",
      "1/1 [==============================] - 0s 258ms/step\n",
      "1/1 [==============================] - 0s 259ms/step\n",
      "1/1 [==============================] - 0s 264ms/step\n",
      "1/1 [==============================] - 0s 354ms/step\n",
      "1/1 [==============================] - 0s 253ms/step\n",
      "1/1 [==============================] - 0s 253ms/step\n",
      "1/1 [==============================] - 1s 743ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Training Time (s)</th>\n",
       "      <th>Trainable Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Fine-Tuning</td>\n",
       "      <td>0.2225</td>\n",
       "      <td>0.098889</td>\n",
       "      <td>0.2225</td>\n",
       "      <td>0.084762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>66957317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Standard Fine-Tuning</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>278.466017</td>\n",
       "      <td>66957317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Accuracy  Precision  Recall  F1 Score  \\\n",
       "0        No Fine-Tuning    0.2225   0.098889  0.2225  0.084762   \n",
       "1  Standard Fine-Tuning    1.0000   1.000000  1.0000  1.000000   \n",
       "\n",
       "   Training Time (s)  Trainable Parameters  \n",
       "0           0.000000              66957317  \n",
       "1         278.466017              66957317  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform prediction on the test dataset\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Iterate over test dataset to collect true labels and predictions\n",
    "for batch in test_dataset:\n",
    "    input_data, labels = batch\n",
    "    y_true.extend(labels.numpy())  # Collect true labels\n",
    "    logits = std_ft_model.predict(input_data).logits  # Predict logits\n",
    "    predictions = logits.argmax(axis=-1)  # Convert logits to predicted labels\n",
    "    y_pred.extend(predictions)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "std_ft_accuracy, std_ft_precision, std_ft_recall, std_ft_f1 = evaluate_model(y_true, y_pred)\n",
    "std_ft_training_time = std_ft_end_time - std_ft_start_time\n",
    "\n",
    "# Find number of trainable parameters\n",
    "trainable_params = 0\n",
    "for variable in std_ft_model.trainable_weights:\n",
    "    # Use the 'count_params' method to get the number of parameters in each variable\n",
    "    param_count = tf.keras.backend.count_params(variable)\n",
    "    trainable_params += param_count\n",
    "no_finetuning_trainable_params = trainable_params\n",
    "\n",
    "\n",
    "\n",
    "model_metrics = [\n",
    "    {\n",
    "        'Model': 'No Fine-Tuning',\n",
    "        'Accuracy': no_finetuning_accuracy,\n",
    "        'Precision': no_finetuning_precision,\n",
    "        'Recall': no_finetuning_recall,\n",
    "        'F1 Score': no_finetuning_f1,\n",
    "        'Training Time (s)': 0,\n",
    "        'Trainable Parameters': no_finetuning_trainable_params\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Standard Fine-Tuning',\n",
    "        'Accuracy': std_ft_accuracy,\n",
    "        'Precision': std_ft_precision,\n",
    "        'Recall': std_ft_recall,\n",
    "        'F1 Score': std_ft_f1,\n",
    "        'Training Time (s)': std_ft_training_time,\n",
    "        'Trainable Parameters': trainable_params\n",
    "    },\n",
    "\n",
    "]\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df_metrics = pd.DataFrame(model_metrics)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning With LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "from typing import List\n",
    "\n",
    "class LoraLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_layer,\n",
    "        rank: int = 8,\n",
    "        alpha: int = 32,\n",
    "        dim: int = 768,\n",
    "        dropout: float = 0.05,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # We want to keep the name of this layer the same as the original\n",
    "        # dense layer.\n",
    "        original_layer_config = original_layer.get_config()\n",
    "        name = original_layer_config[\"name\"]\n",
    "\n",
    "        kwargs.pop(\"name\", None)\n",
    "\n",
    "        super().__init__(name=name, **kwargs)\n",
    "\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self._scale = alpha / rank\n",
    "        self.dim = dim  # dim of DistilBert hidden states.\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Layers.\n",
    "\n",
    "        # Original dense layer.\n",
    "        self.original_layer = original_layer\n",
    "        # No matter whether we are training the model or are in inference mode,\n",
    "        # this layer should be frozen.\n",
    "        self.original_layer.trainable = False\n",
    "\n",
    "        # LoRA dense layers.\n",
    "        self.A = tf.keras.layers.Dense(\n",
    "            units=rank,\n",
    "            use_bias=False,\n",
    "            # Note: the original paper mentions that normal distribution was\n",
    "            # used for initialization. However, the official LoRA implementation\n",
    "            # uses \"Kaiming/He Initialization\".\n",
    "            kernel_initializer=tf.keras.initializers.VarianceScaling(scale=math.sqrt(5), mode=\"fan_in\", distribution=\"uniform\"),\n",
    "            name=\"lora_A\",\n",
    "        )\n",
    "\n",
    "        self.B = tf.keras.layers.Dense(\n",
    "            units=self.dim,\n",
    "            use_bias=False,\n",
    "            kernel_initializer=\"zeros\",\n",
    "            name=\"lora_B\",\n",
    "        )\n",
    "\n",
    "        self.dropout_layer = tf.keras.layers.Dropout(self.dropout)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n",
    "        original_output = self.original_layer(inputs)\n",
    "\n",
    "        x = self.A(inputs)\n",
    "        x = self.dropout_layer(x)\n",
    "        lora_output = self.B(x) * self._scale\n",
    "\n",
    "        return original_output + lora_output\n",
    "\n",
    "DISTILBERT_LINEAR_MODULES_DICT = {\n",
    "    \"q_lin\": {\"parent_layer\": \"attention\", \"input_dim\": 768, \"dim\": 768},\n",
    "    \"v_lin\": {\"parent_layer\": \"attention\", \"input_dim\": 768, \"dim\": 768},\n",
    "    \"k_lin\": {\"parent_layer\": \"attention\", \"input_dim\": 768, \"dim\": 768},\n",
    "    \"out_lin\": {\"parent_layer\": \"attention\", \"input_dim\": 768, \"dim\": 768},\n",
    "    \"lin1\": {\"parent_layer\": \"ffn\", \"input_dim\": 768, \"dim\": 3072},\n",
    "    \"lin2\": {\"parent_layer\": \"ffn\", \"input_dim\": 3072, \"dim\": 768},\n",
    "}\n",
    "\n",
    "LORA_PARAMETERS = {\n",
    "    \"rank\": 8,\n",
    "    \"alpha\": 8,\n",
    "    \"target_modules\": [\"q_lin\", \"v_lin\", \"k_lin\", \"out_lin\", \"lin1\", \"lin2\"],\n",
    "    \"dropout\": 0.05,\n",
    "}\n",
    "\n",
    "def apply_lora(\n",
    "    model,\n",
    "    rank: int,\n",
    "    alpha: int,\n",
    "    target_modules: List[str],\n",
    "    dropout: float = 0.05\n",
    "):\n",
    "    for i in range(model.distilbert.transformer.n_layers):\n",
    "        for target_module in target_modules:\n",
    "            parent_layer_name = DISTILBERT_LINEAR_MODULES_DICT[target_module][\"parent_layer\"]\n",
    "            parent_layer = getattr(\n",
    "                model.distilbert.transformer.layer[i],\n",
    "                parent_layer_name,\n",
    "            )\n",
    "\n",
    "            original_target_layer = getattr(parent_layer, target_module)\n",
    "            original_target_layer_dim = DISTILBERT_LINEAR_MODULES_DICT[target_module][\"dim\"]\n",
    "\n",
    "            lora_layer = LoraLayer(\n",
    "                original_layer=original_target_layer,\n",
    "                rank=rank,\n",
    "                alpha=alpha,\n",
    "                trainable=True,\n",
    "                dim=original_target_layer_dim,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            setattr(parent_layer, target_module, lora_layer)\n",
    "\n",
    "            input_dim = DISTILBERT_LINEAR_MODULES_DICT[target_module][\"input_dim\"]\n",
    "            getattr(parent_layer, target_module).A.build(input_dim)\n",
    "            getattr(parent_layer, target_module).B.build(rank)\n",
    "\n",
    "    # Set all distilbert linear layers to trainable=False except the LoRA layers\n",
    "    model.distilbert.embeddings.trainable = False\n",
    "    for (layer) in (model.distilbert._flatten_layers()):\n",
    "        lst_of_sublayers = list(layer._flatten_layers())\n",
    "\n",
    "        if len(lst_of_sublayers) == 1:  # \"leaves of the model\"\n",
    "            if layer.name in [\"lora_A\", \"lora_B\"]:\n",
    "                layer.trainable = True\n",
    "            else:\n",
    "                layer.trainable = False\n",
    "\n",
    "    return model\n",
    "\n",
    "def merge_lora_weights(\n",
    "    model,\n",
    "    rank: int,\n",
    "    alpha: int,\n",
    "    target_modules: List[str]\n",
    "):\n",
    "\n",
    "    scale = alpha / rank\n",
    "    for i in range(model.distilbert.transformer.n_layers):\n",
    "        for target_module in target_modules:\n",
    "            parent_layer_name = DISTILBERT_LINEAR_MODULES_DICT[target_module][\"parent_layer\"]\n",
    "            parent_layer = getattr(\n",
    "                model.distilbert.transformer.layer[i],\n",
    "                parent_layer_name,\n",
    "            )\n",
    "\n",
    "            target_layer = getattr(parent_layer, target_module)\n",
    "            target_layer_input_dim = DISTILBERT_LINEAR_MODULES_DICT[target_module][\"input_dim\"]\n",
    "\n",
    "            A_layer = getattr(target_layer, \"A\")\n",
    "            B_layer = getattr(target_layer, \"B\")\n",
    "            original_dense_layer = getattr(target_layer, \"original_layer\")\n",
    "\n",
    "            lora_weights = tf.linalg.matmul(A_layer.kernel, B_layer.kernel)\n",
    "            original_dense_layer_weights = original_dense_layer.kernel\n",
    "\n",
    "            merged_layer_weights = original_dense_layer_weights + lora_weights * scale\n",
    "            merged_layer_bias = original_dense_layer.bias\n",
    "\n",
    "            merged_layer = tf.keras.layers.Dense(\n",
    "                units=original_dense_layer.units,\n",
    "                kernel_initializer=tf.constant_initializer(merged_layer_weights.numpy()),\n",
    "                bias_initializer=tf.constant_initializer(merged_layer_bias.numpy()),\n",
    "                name=target_module,\n",
    "            )\n",
    "            merged_layer.build(target_layer_input_dim)\n",
    "\n",
    "            setattr(parent_layer, target_module, merged_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 3\n",
    "batches_per_epoch = len(train_dataset)\n",
    "total_train_steps = batches_per_epoch * num_epochs\n",
    "\n",
    "TRAINING_PARAMETERS = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"batches_per_epoch\": batches_per_epoch,\n",
    "    \"total_train_steps\": total_train_steps,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"num_warmup_steps\": 0,\n",
    "}\n",
    "\n",
    "def train(train_dataset, validation_dataset, model, training_parameters, lora_parameters) -> None:\n",
    "\n",
    "    if lora_parameters:\n",
    "        print(\"Model summary before applying LoRA layers:\")\n",
    "        print(model.summary())\n",
    "        print()\n",
    "        model = apply_lora(model, **lora_parameters)\n",
    "        print(\"Model summary after applying LoRA layers:\")\n",
    "        print(model.summary())\n",
    "        print()\n",
    "\n",
    "    optimizer = tf_keras.optimizers.legacy.Adam(learning_rate=5e-5)\n",
    "    model.compile(optimizer=optimizer)\n",
    "\n",
    "    # metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=validation_dataset)\n",
    "\n",
    "    loRA_ft_start_time = time.time()\n",
    "    model.fit(\n",
    "        x=train_dataset,\n",
    "        validation_data=validation_dataset,\n",
    "        epochs=training_parameters[\"num_epochs\"],\n",
    "        # callbacks=[metric_callback]\n",
    "    )\n",
    "    loRA_ft_end_time = time.time()\n",
    "\n",
    "    loRA_ft_training_time = loRA_ft_end_time - loRA_ft_start_time\n",
    "\n",
    "    if lora_parameters:\n",
    "        model = merge_lora_weights(\n",
    "            model,\n",
    "            rank=lora_parameters[\"rank\"],\n",
    "            alpha=lora_parameters[\"alpha\"],\n",
    "            target_modules=lora_parameters[\"target_modules\"]\n",
    "        )\n",
    "        print()\n",
    "        print(\"Model summary after merging LoRA weights:\")\n",
    "        print(model.summary())\n",
    "        print()\n",
    "        evaluation = model.evaluate(validation_dataset, return_dict=True)\n",
    "        print(\"Evaluation after merging weights:\", evaluation)\n",
    "\n",
    "    return model, loRA_ft_training_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary before applying LoRA layers:\n",
      "Model: \"tf_distil_bert_for_sequence_classification_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMa  multiple                  66362880  \n",
      " inLayer)                                                        \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  3845      \n",
      "                                                                 \n",
      " dropout_531 (Dropout)       multiple                  0 (unused)\n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66957317 (255.42 MB)\n",
      "Trainable params: 66957317 (255.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Model summary after applying LoRA layers:\n",
      "Model: \"tf_distil_bert_for_sequence_classification_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMa  multiple                  67026432  \n",
      " inLayer)                                                        \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  3845      \n",
      "                                                                 \n",
      " dropout_531 (Dropout)       multiple                  0 (unused)\n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67620869 (257.95 MB)\n",
      "Trainable params: 1257989 (4.80 MB)\n",
      "Non-trainable params: 66362880 (253.15 MB)\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Epoch 1/3\n",
      "113/113 [==============================] - 80s 661ms/step - loss: 1.1174 - val_loss: 0.3057\n",
      "Epoch 2/3\n",
      "113/113 [==============================] - 74s 657ms/step - loss: 0.1798 - val_loss: 0.0700\n",
      "Epoch 3/3\n",
      "113/113 [==============================] - 77s 682ms/step - loss: 0.0514 - val_loss: 0.0202\n",
      "\n",
      "Model summary after merging LoRA weights:\n",
      "Model: \"tf_distil_bert_for_sequence_classification_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMa  multiple                  66362880  \n",
      " inLayer)                                                        \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  3845      \n",
      "                                                                 \n",
      " dropout_531 (Dropout)       multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66957317 (255.42 MB)\n",
      "Trainable params: 43103237 (164.43 MB)\n",
      "Non-trainable params: 23854080 (91.00 MB)\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "32/32 [==============================] - 9s 273ms/step - loss: 0.0202\n",
      "Evaluation after merging weights: {'loss': 0.020162587985396385}\n"
     ]
    }
   ],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=5)\n",
    "\n",
    "\n",
    "loRA_ft_model, loRA_ft_training_time = train(\n",
    "    train_dataset=train_dataset,\n",
    "    validation_dataset=val_dataset,\n",
    "    model=model,\n",
    "    training_parameters=TRAINING_PARAMETERS,\n",
    "    lora_parameters=LORA_PARAMETERS,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform prediction on the test dataset\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Iterate over test dataset to collect true labels and predictions\n",
    "for batch in test_dataset:\n",
    "    input_data, labels = batch\n",
    "    y_true.extend(labels.numpy())  # Collect true labels\n",
    "    logits = loRA_ft_model.predict(input_data).logits  # Predict logits\n",
    "    predictions = logits.argmax(axis=-1)  # Convert logits to predicted labels\n",
    "    y_pred.extend(predictions)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "loRA_ft_accuracy, loRA_ft_precision, loRA_ft_recall, loRA_ft_f1 = evaluate_model(y_true, y_pred)\n",
    "\n",
    "\n",
    "# Find number of trainable parameters\n",
    "loRA_ft_trainable_params = 0\n",
    "for variable in loRA_ft_model.trainable_weights:\n",
    "    # Use the 'count_params' method to get the number of parameters in each variable\n",
    "    param_count = tf.keras.backend.count_params(variable)\n",
    "    loRA_ft_trainable_params += param_count\n",
    "loRA_ft_trainable_params = loRA_ft_trainable_params\n",
    "\n",
    "\n",
    "\n",
    "model_metrics = [\n",
    "    {\n",
    "        'Model': 'No Fine-Tuning',\n",
    "        'Accuracy': no_finetuning_accuracy,\n",
    "        'Precision': no_finetuning_precision,\n",
    "        'Recall': no_finetuning_recall,\n",
    "        'F1 Score': no_finetuning_f1,\n",
    "        'Training Time (s)': 0,\n",
    "        'Trainable Parameters': no_finetuning_trainable_params\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Standard Fine-Tuning',\n",
    "        'Accuracy': std_ft_accuracy,\n",
    "        'Precision': std_ft_precision,\n",
    "        'Recall': std_ft_recall,\n",
    "        'F1 Score': std_ft_f1,\n",
    "        'Training Time (s)': std_ft_training_time,\n",
    "        'Trainable Parameters': trainable_params\n",
    "    },\n",
    "        {\n",
    "        'Model': 'LoRA Fine-Tuning',\n",
    "        'Accuracy': loRA_ft_accuracy,\n",
    "        'Precision': loRA_ft_precision,\n",
    "        'Recall': loRA_ft_recall,\n",
    "        'F1 Score': loRA_ft_f1,\n",
    "        'Training Time (s)': loRA_ft_training_time,\n",
    "        'Trainable Parameters': loRA_ft_trainable_params\n",
    "    },\n",
    "\n",
    "]\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df_metrics = pd.DataFrame(model_metrics)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saved_models/tokenizer_config.json',\n",
       " './saved_models/special_tokens_map.json',\n",
       " './saved_models/vocab.txt',\n",
       " './saved_models/added_tokens.json')"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_directory = \"./saved_models\" \n",
    "\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
