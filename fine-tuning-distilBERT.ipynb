{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune DistilBERT For Multi-Class Text Classification Using Tensorflow and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we explore the capabilities of DistilBERT for multi-class text classification by comparing three approaches: no fine-tuning, standard fine-tuning, and LoRA fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents:\n",
    "\n",
    "    1. Data Pre-Processing\n",
    "        1.1. Split into Train, Validation, Test using Stratified Sampling\n",
    "        1.2. View test/train/validation Splits\n",
    "        1.3. (Optional) Save test/train/val to CSV\n",
    "        1.4. Tokenize Data For DistilBERT Model\n",
    "    \n",
    "    2. Model Evaluation\n",
    "        2.1. Model Parameters\n",
    "        2.2. Model Metrics\n",
    "\n",
    "    3. DistilBERT with No Fine-Tuning\n",
    "        3.1. Model Setup\n",
    "        3.2. Model Evaluation\n",
    "\n",
    "    4. DistilBERT with Standard Fine-Tuning\n",
    "        4.1. Model Setup\n",
    "        4.2. Model Training\n",
    "        4.3. Model Evaluation\n",
    "\n",
    "    5. DistilBERT with LoRA Fine-Tuning\n",
    "        5.1. Implementing the 'LoRALayer' class\n",
    "        5.2. Applying LoRA\n",
    "        5.3. Defining the LoRALayer class\n",
    "        5.4. Model Training\n",
    "        5.5. Model Evaluation\n",
    "\n",
    "    6. Saving the Model\n",
    "        6.1. (Optional) Merging LoRA Weights\n",
    "        6.2. Saving the Model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import required packages\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "import tf_keras\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Split into Train, Validation, Test using Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "root_path = 'data/full_dataset.csv'\n",
    "df = pd.read_csv(root_path)\n",
    "df.head()\n",
    "\n",
    "# Encode the 'category' column into numerical labels\n",
    "df['encoded_text'] = df['category'].astype('category').cat.codes\n",
    "\n",
    "# Separate columns for splitting\n",
    "data_texts = df['request'].to_list()  # 'request' is the text data\n",
    "data_labels = df['encoded_text'].to_list()  # Encoded class labels\n",
    "stratify_values = df['stratify_col'].to_list()  # Stratification column\n",
    "\n",
    "# Split the data into Train/Validation sets with stratification\n",
    "train_texts, val_texts, train_labels, val_labels, train_stratify, val_stratify = train_test_split(\n",
    "    data_texts, data_labels, stratify_values, \n",
    "    test_size=0.2, stratify=stratify_values, random_state=0\n",
    ")\n",
    "\n",
    "# Split the Train set further into Train/Test with stratification\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    train_texts, train_labels, \n",
    "    test_size=0.1, stratify=train_stratify, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. View test/train/validation Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label Mapping (Encoded -> Category):\n",
      "0: Facilities Management\n",
      "1: Finance\n",
      "2: HR\n",
      "3: IT Support\n",
      "4: Marketing\n",
      "\n",
      "Final dataset information:\n",
      "Train set size: 3596\n",
      "Validation set size: 1000\n",
      "Test set size: 400\n",
      "Example train_texts: ['I’m gathering details about rewards for long-term employees and was hoping you could provide some insight. Let me know if you need further specifics from me.', 'Do you have the latest version of the diversity and inclusion policies handbook? I need it for a new hire orientation.', 'Could you share detailed insights on the performance metrics for keyword research for PPC campaigns? I’d like to use this data for our planning.']\n",
      "Example train_labels: [2, 2, 4]\n",
      "Example val_texts: ['Could you outline the steps to optimize our launching retargeting ads approach? Any case studies or examples would be helpful.', 'Need access to server maintenance.', 'Insights on customer retention strategies performance needed.']\n",
      "Example val_labels: [4, 3, 4]\n",
      "Example test_texts: ['I’m experiencing an issue with software installation. Can you assist in diagnosing and resolving the problem as soon as possible?', 'Could you review and approve the budget variance analysis for this month? Let me know if there’s anything I need to address.', 'I wanted to check on the status of my request for access to storage capacity planning. Let me know if further details are needed.']\n",
      "Example test_labels: [3, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "# Map numerical labels back to category names\n",
    "label_mapping = dict(enumerate(df['category'].astype('category').cat.categories))\n",
    "print(\"\\nLabel Mapping (Encoded -> Category):\")\n",
    "\n",
    "for encoded, category in label_mapping.items():\n",
    "    print(f\"{encoded}: {category}\")\n",
    "\n",
    "# Output dataset information\n",
    "print(\"\\nFinal dataset information:\")\n",
    "print(f\"Train set size: {len(train_texts)}\")\n",
    "print(f\"Validation set size: {len(val_texts)}\")\n",
    "print(f\"Test set size: {len(test_texts)}\")\n",
    "\n",
    "print(f\"Example train_texts: {train_texts[:3]}\") \n",
    "print(f\"Example train_labels: {train_labels[:3]}\")\n",
    "print(f\"Example val_texts: {val_texts[:3]}\") \n",
    "print(f\"Example val_labels: {val_labels[:3]}\")\n",
    "print(f\"Example test_texts: {test_texts[:3]}\") \n",
    "print(f\"Example test_labels: {test_labels[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. (Optional) Save test/train/val to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames for each split\n",
    "train_df = pd.DataFrame({\n",
    "    'request': train_texts,\n",
    "    'label': train_labels\n",
    "})\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    'request': val_texts,\n",
    "    'label': val_labels\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'request': test_texts,\n",
    "    'label': test_labels\n",
    "})\n",
    "\n",
    "# Save DataFrames to CSV files\n",
    "train_df.to_csv(\"data/train.csv\", index=False)\n",
    "val_df.to_csv(\"data/validation.csv\", index=False)\n",
    "test_df.to_csv(\"data/test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Tokenize Data For DistilBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation = True, padding = True  )\n",
    "\n",
    "val_encodings = tokenizer(val_texts, truncation = True, padding = True )\n",
    "\n",
    "test_encodings = tokenizer(test_texts, truncation = True, padding = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    ")).batch(32)\n",
    "\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    ")).batch(32)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    test_labels\n",
    ")).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our three models (No Fine-tune, Standard fine-tune, LoRA fine tune), we will be using the DistilBERT model, an Adam optimize function, SparseCategoricalCrossentropy, a batch size of 32 and 3 ephocs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initial Model setup\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=5)\n",
    "opt = tf_keras.optimizers.legacy.Adam(learning_rate=5e-5)\n",
    "loss = tf_keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Raw logits expected\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 3\n",
    "batches_per_epoch = len(train_dataset)\n",
    "total_train_steps = batches_per_epoch * num_epochs\n",
    "\n",
    "TRAINING_PARAMETERS = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"batches_per_epoch\": batches_per_epoch,\n",
    "    \"total_train_steps\": total_train_steps,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"num_warmup_steps\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Model Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of our three models, we will collect the following metrics:\n",
    "    1.    accuracy\n",
    "    2.    precision\n",
    "    3.    recall\n",
    "    4.    f1\n",
    "    5.    Training Time\n",
    "    6.    Trainable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate_model(model):\n",
    "    # Perform prediction on the test dataset\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    # Iterate over test dataset to collect true labels and predictions\n",
    "    for batch in test_dataset:\n",
    "        input_data, labels = batch\n",
    "        y_true.extend(labels.numpy())  # Collect true labels\n",
    "        logits = model.predict(input_data).logits  # Predict logits\n",
    "        predictions = logits.argmax(axis=-1)  # Convert logits to predicted labels\n",
    "        y_pred.extend(predictions)\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    trainable_params = np.sum([np.prod(v.get_shape().as_list()) for v in model.trainable_variables])\n",
    "    return accuracy, precision, recall, f1, trainable_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following function to display our models metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the metrics list\n",
    "model_metrics = []\n",
    "\n",
    "# Define a function to add metrics\n",
    "def add_model_metrics(model_name, accuracy, precision, recall, f1, training_time, trainable_params):\n",
    "    model_metrics.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Training Time (s)': training_time,\n",
    "        'Trainable Parameters': trainable_params\n",
    "    })\n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    df_metrics = pd.DataFrame(model_metrics)\n",
    "    display(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DistilBERT With No Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the non-fine tuned model, we will simply compile the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_ft_model = model\n",
    "no_ft_model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 641ms/step\n",
      "1/1 [==============================] - 0s 243ms/step\n",
      "1/1 [==============================] - 0s 240ms/step\n",
      "1/1 [==============================] - 0s 235ms/step\n",
      "1/1 [==============================] - 0s 238ms/step\n",
      "1/1 [==============================] - 0s 235ms/step\n",
      "1/1 [==============================] - 0s 239ms/step\n",
      "1/1 [==============================] - 0s 236ms/step\n",
      "1/1 [==============================] - 0s 236ms/step\n",
      "1/1 [==============================] - 0s 237ms/step\n",
      "1/1 [==============================] - 0s 239ms/step\n",
      "1/1 [==============================] - 0s 236ms/step\n",
      "1/1 [==============================] - 1s 714ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bennettbishop/enterprise-email-routing/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Training Time (s)</th>\n",
       "      <th>Trainable Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Fine-Tuning</td>\n",
       "      <td>0.2425</td>\n",
       "      <td>0.114557</td>\n",
       "      <td>0.2425</td>\n",
       "      <td>0.142696</td>\n",
       "      <td>0</td>\n",
       "      <td>66957317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model  Accuracy  Precision  Recall  F1 Score  Training Time (s)  \\\n",
       "0  No Fine-Tuning    0.2425   0.114557  0.2425  0.142696                  0   \n",
       "\n",
       "   Trainable Parameters  \n",
       "0              66957317  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "no_ft_accuracy, no_ft_precision, no_ft_recall, no_ft_f1, no_ft_trainable_params = evaluate_model(no_ft_model)\n",
    "no_ft_training_time = 0, #There is no training involved, as we are not fine tuning the model\n",
    "\n",
    "# Add metrics for \"No Fine-Tuning\"\n",
    "no_ft_training_time = 0  # No training involved\n",
    "add_model_metrics(\n",
    "    'No Fine-Tuning',\n",
    "    no_ft_accuracy, no_ft_precision, no_ft_recall, no_ft_f1,\n",
    "    no_ft_training_time, no_ft_trainable_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict Categories With Standard Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_ft_model = model\n",
    "std_ft_model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "113/113 [==============================] - 99s 855ms/step - loss: 0.0229 - accuracy: 0.9944 - val_loss: 0.0054 - val_accuracy: 0.9980\n",
      "Epoch 2/3\n",
      "113/113 [==============================] - 97s 855ms/step - loss: 0.0102 - accuracy: 0.9975 - val_loss: 0.0053 - val_accuracy: 0.9990\n",
      "Epoch 3/3\n",
      "113/113 [==============================] - 97s 857ms/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 7.5855e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "std_ft_start_time = time.time()\n",
    "std_ft_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=TRAINING_PARAMETERS[\"num_epochs\"],\n",
    "    verbose=1,\n",
    ") \n",
    "std_ft_end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 682ms/step\n",
      "1/1 [==============================] - 0s 258ms/step\n",
      "1/1 [==============================] - 0s 256ms/step\n",
      "1/1 [==============================] - 0s 260ms/step\n",
      "1/1 [==============================] - 0s 290ms/step\n",
      "1/1 [==============================] - 0s 286ms/step\n",
      "1/1 [==============================] - 0s 263ms/step\n",
      "1/1 [==============================] - 0s 252ms/step\n",
      "1/1 [==============================] - 0s 264ms/step\n",
      "1/1 [==============================] - 0s 282ms/step\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "1/1 [==============================] - 0s 284ms/step\n",
      "1/1 [==============================] - 1s 862ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Training Time (s)</th>\n",
       "      <th>Trainable Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Fine-Tuning</td>\n",
       "      <td>0.2425</td>\n",
       "      <td>0.114557</td>\n",
       "      <td>0.2425</td>\n",
       "      <td>0.142696</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>66957317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Standard Fine-Tuning</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>292.727587</td>\n",
       "      <td>66957317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Accuracy  Precision  Recall  F1 Score  \\\n",
       "0        No Fine-Tuning    0.2425   0.114557  0.2425  0.142696   \n",
       "1  Standard Fine-Tuning    1.0000   1.000000  1.0000  1.000000   \n",
       "\n",
       "   Training Time (s)  Trainable Parameters  \n",
       "0           0.000000              66957317  \n",
       "1         292.727587              66957317  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "std_ft_accuracy, std_ft_precision, std_ft_recall, std_ft_f1, std_ft_trainable_params = evaluate_model(std_ft_model)\n",
    "std_ft_training_time = std_ft_end_time - std_ft_start_time # Find training time\n",
    "\n",
    "# Add metrics for \"Standard Fine-Tuning\"\n",
    "std_ft_training_time = std_ft_end_time - std_ft_start_time  # Calculate training time\n",
    "add_model_metrics(\n",
    "    'Standard Fine-Tuning',\n",
    "    std_ft_accuracy, std_ft_precision, std_ft_recall, std_ft_f1,\n",
    "    std_ft_training_time, std_ft_trainable_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DistilBERT With LoRA Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Implementing the LoRALayer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must define the LoRA layer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List\n",
    "\n",
    "class LoraLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_layer,\n",
    "        rank: int = 8,\n",
    "        alpha: int = 32,\n",
    "        dim: int = 768,\n",
    "        dropout: float = 0.05,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # We want to keep the name of this layer the same as the original\n",
    "        # dense layer.\n",
    "        original_layer_config = original_layer.get_config()\n",
    "        name = original_layer_config[\"name\"]\n",
    "\n",
    "        kwargs.pop(\"name\", None)\n",
    "\n",
    "        super().__init__(name=name, **kwargs)\n",
    "\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self._scale = alpha / rank\n",
    "        self.dim = dim  # dim of DistilBert hidden states.\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Layers.\n",
    "\n",
    "        # Original dense layer.\n",
    "        self.original_layer = original_layer\n",
    "        # No matter whether we are training the model or are in inference mode,\n",
    "        # this layer should be frozen.\n",
    "        self.original_layer.trainable = False\n",
    "\n",
    "        # LoRA dense layers.\n",
    "        self.A = tf.keras.layers.Dense(\n",
    "            units=rank,\n",
    "            use_bias=False,\n",
    "            # Note: the original paper mentions that normal distribution was\n",
    "            # used for initialization. However, the official LoRA implementation\n",
    "            # uses \"Kaiming/He Initialization\".\n",
    "            kernel_initializer=tf.keras.initializers.VarianceScaling(scale=math.sqrt(5), mode=\"fan_in\", distribution=\"uniform\"),\n",
    "            name=\"lora_A\",\n",
    "        )\n",
    "\n",
    "        self.B = tf.keras.layers.Dense(\n",
    "            units=self.dim,\n",
    "            use_bias=False,\n",
    "            kernel_initializer=\"zeros\",\n",
    "            name=\"lora_B\",\n",
    "        )\n",
    "\n",
    "        self.dropout_layer = tf.keras.layers.Dropout(self.dropout)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n",
    "        original_output = self.original_layer(inputs)\n",
    "\n",
    "        x = self.A(inputs)\n",
    "        x = self.dropout_layer(x)\n",
    "        lora_output = self.B(x) * self._scale\n",
    "\n",
    "        return original_output + lora_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will write the script to apply LoRA to the wanted layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Applying LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DISTILBERT_LINEAR_MODULES_DICT = {\n",
    "    \"q_lin\": {\"parent_layer\": \"attention\", \"input_dim\": 768, \"dim\": 768},\n",
    "    \"v_lin\": {\"parent_layer\": \"attention\", \"input_dim\": 768, \"dim\": 768},\n",
    "    \"k_lin\": {\"parent_layer\": \"attention\", \"input_dim\": 768, \"dim\": 768},\n",
    "    \"out_lin\": {\"parent_layer\": \"attention\", \"input_dim\": 768, \"dim\": 768},\n",
    "    \"lin1\": {\"parent_layer\": \"ffn\", \"input_dim\": 768, \"dim\": 3072},\n",
    "    \"lin2\": {\"parent_layer\": \"ffn\", \"input_dim\": 3072, \"dim\": 768},\n",
    "}\n",
    "\n",
    "LORA_PARAMETERS = {\n",
    "    \"rank\": 8,\n",
    "    \"alpha\": 8,\n",
    "    \"target_modules\": [\"q_lin\", \"v_lin\", \"k_lin\", \"out_lin\", \"lin1\", \"lin2\"],\n",
    "    \"dropout\": 0.05,\n",
    "}\n",
    "\n",
    "def apply_lora(\n",
    "    model,\n",
    "    rank: int,\n",
    "    alpha: int,\n",
    "    target_modules: List[str],\n",
    "    dropout: float = 0.05\n",
    "):\n",
    "    for i in range(model.distilbert.transformer.n_layers):\n",
    "        for target_module in target_modules:\n",
    "            parent_layer_name = DISTILBERT_LINEAR_MODULES_DICT[target_module][\"parent_layer\"]\n",
    "            parent_layer = getattr(\n",
    "                model.distilbert.transformer.layer[i],\n",
    "                parent_layer_name,\n",
    "            )\n",
    "\n",
    "            original_target_layer = getattr(parent_layer, target_module)\n",
    "            original_target_layer_dim = DISTILBERT_LINEAR_MODULES_DICT[target_module][\"dim\"]\n",
    "\n",
    "            lora_layer = LoraLayer(\n",
    "                original_layer=original_target_layer,\n",
    "                rank=rank,\n",
    "                alpha=alpha,\n",
    "                trainable=True,\n",
    "                dim=original_target_layer_dim,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            setattr(parent_layer, target_module, lora_layer)\n",
    "\n",
    "            input_dim = DISTILBERT_LINEAR_MODULES_DICT[target_module][\"input_dim\"]\n",
    "            getattr(parent_layer, target_module).A.build(input_dim)\n",
    "            getattr(parent_layer, target_module).B.build(rank)\n",
    "\n",
    "    # Set all distilbert linear layers to trainable=False except the LoRA layers\n",
    "    model.distilbert.embeddings.trainable = False\n",
    "    for (layer) in (model.distilbert._flatten_layers()):\n",
    "        lst_of_sublayers = list(layer._flatten_layers())\n",
    "\n",
    "        if len(lst_of_sublayers) == 1:  # \"leaves of the model\"\n",
    "            if layer.name in [\"lora_A\", \"lora_B\"]:\n",
    "                layer.trainable = True\n",
    "            else:\n",
    "                layer.trainable = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "loRA_ft_model = apply_lora(model, **LORA_PARAMETERS)\n",
    "loRA_ft_model.compile(optimizer=opt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "113/113 [==============================] - 83s 694ms/step - loss: 7.0694e-04 - val_loss: 4.6793e-04\n",
      "Epoch 2/3\n",
      "113/113 [==============================] - 79s 702ms/step - loss: 4.8847e-04 - val_loss: 3.0773e-04\n",
      "Epoch 3/3\n",
      "113/113 [==============================] - 78s 689ms/step - loss: 3.7987e-04 - val_loss: 2.5724e-04\n"
     ]
    }
   ],
   "source": [
    "loRA_ft_start_time = time.time()\n",
    "loRA_ft_model.fit(\n",
    "    x=train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=TRAINING_PARAMETERS[\"num_epochs\"],\n",
    ")\n",
    "loRA_ft_end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 273ms/step\n",
      "1/1 [==============================] - 0s 268ms/step\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "1/1 [==============================] - 0s 341ms/step\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "1/1 [==============================] - 0s 282ms/step\n",
      "1/1 [==============================] - 0s 282ms/step\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "1/1 [==============================] - 0s 275ms/step\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Training Time (s)</th>\n",
       "      <th>Trainable Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Fine-Tuning</td>\n",
       "      <td>0.2425</td>\n",
       "      <td>0.114557</td>\n",
       "      <td>0.2425</td>\n",
       "      <td>0.142696</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>66957317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Standard Fine-Tuning</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>292.727587</td>\n",
       "      <td>66957317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LoRA Fine-Tuning</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>240.234911</td>\n",
       "      <td>1257989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LoRA Fine-Tuning</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>240.234911</td>\n",
       "      <td>1257989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Accuracy  Precision  Recall  F1 Score  \\\n",
       "0        No Fine-Tuning    0.2425   0.114557  0.2425  0.142696   \n",
       "1  Standard Fine-Tuning    1.0000   1.000000  1.0000  1.000000   \n",
       "2      LoRA Fine-Tuning    1.0000   1.000000  1.0000  1.000000   \n",
       "3      LoRA Fine-Tuning    1.0000   1.000000  1.0000  1.000000   \n",
       "\n",
       "   Training Time (s)  Trainable Parameters  \n",
       "0           0.000000              66957317  \n",
       "1         292.727587              66957317  \n",
       "2         240.234911               1257989  \n",
       "3         240.234911               1257989  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loRA_ft_accuracy, loRA_ft_precision, loRA_ft_recall, loRA_ft_f1, loRA_ft_trainable_params = evaluate_model(loRA_ft_model)\n",
    "loRA_ft_training_time = loRA_ft_end_time - loRA_ft_start_time # Find the training time\n",
    "\n",
    "# Add metrics for \"LoRA Fine-Tuning\"\n",
    "add_model_metrics(\n",
    "    'LoRA Fine-Tuning',\n",
    "    loRA_ft_accuracy, loRA_ft_precision, loRA_ft_recall, loRA_ft_f1,\n",
    "    loRA_ft_training_time, loRA_ft_trainable_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. (Optional) Merging LoRA Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRA is an excellent approach for managing multiple fine-tuned models efficiently. However, in this case, we only need to handle a single downstream task: classifying texts into categories. To optimize inference performance during deployment, we will merge the LoRA layers with the base model and save the resulting model. While this process increases the total number of parameters, it is irrelevant since the model will be used exclusively for inference, not training. Merging the layers reduces inference latency, making this approach the most suitable for our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lora_weights(\n",
    "    model,\n",
    "    rank: int,\n",
    "    alpha: int,\n",
    "    target_modules: List[str]\n",
    "):\n",
    "\n",
    "    scale = alpha / rank\n",
    "    for i in range(model.distilbert.transformer.n_layers):\n",
    "        for target_module in target_modules:\n",
    "            parent_layer_name = DISTILBERT_LINEAR_MODULES_DICT[target_module][\"parent_layer\"]\n",
    "            parent_layer = getattr(\n",
    "                model.distilbert.transformer.layer[i],\n",
    "                parent_layer_name,\n",
    "            )\n",
    "\n",
    "            target_layer = getattr(parent_layer, target_module)\n",
    "            target_layer_input_dim = DISTILBERT_LINEAR_MODULES_DICT[target_module][\"input_dim\"]\n",
    "\n",
    "            A_layer = getattr(target_layer, \"A\")\n",
    "            B_layer = getattr(target_layer, \"B\")\n",
    "            original_dense_layer = getattr(target_layer, \"original_layer\")\n",
    "\n",
    "            lora_weights = tf.linalg.matmul(A_layer.kernel, B_layer.kernel)\n",
    "            original_dense_layer_weights = original_dense_layer.kernel\n",
    "\n",
    "            merged_layer_weights = original_dense_layer_weights + lora_weights * scale\n",
    "            merged_layer_bias = original_dense_layer.bias\n",
    "\n",
    "            merged_layer = tf.keras.layers.Dense(\n",
    "                units=original_dense_layer.units,\n",
    "                kernel_initializer=tf.constant_initializer(merged_layer_weights.numpy()),\n",
    "                bias_initializer=tf.constant_initializer(merged_layer_bias.numpy()),\n",
    "                name=target_module,\n",
    "            )\n",
    "            merged_layer.build(target_layer_input_dim)\n",
    "\n",
    "            setattr(parent_layer, target_module, merged_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_model = merge_lora_weights(\n",
    "    model,\n",
    "    rank=LORA_PARAMETERS[\"rank\"],\n",
    "    alpha=LORA_PARAMETERS[\"alpha\"],\n",
    "    target_modules=LORA_PARAMETERS[\"target_modules\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saved_models/tokenizer_config.json',\n",
       " './saved_models/special_tokens_map.json',\n",
       " './saved_models/vocab.txt',\n",
       " './saved_models/added_tokens.json')"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_directory = \"./saved_models\" \n",
    "\n",
    "merged_model.save_pretrained(save_directory)\n",
    "\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
