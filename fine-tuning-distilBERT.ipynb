{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune DistilBERT For Multi-Class Text Classification Using Tensorflow and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import required packages\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.offline import iplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Train, Validation, Test using Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique categories: 5\n",
      "Encoded labels: [2 1 3 4 0]\n"
     ]
    }
   ],
   "source": [
    "# Import Data\n",
    "root_path = 'data/full_dataset.csv'\n",
    "df = pd.read_csv(root_path)\n",
    "df.head()\n",
    "\n",
    "# Encode the 'category' column into numerical labels\n",
    "df['encoded_text'] = df['category'].astype('category').cat.codes\n",
    "\n",
    "# Separate columns for splitting\n",
    "data_texts = df['request'].to_list()  # 'request' is the text data\n",
    "data_labels = df['encoded_text'].to_list()  # Encoded class labels\n",
    "stratify_values = df['stratify_col'].to_list()  # Stratification column\n",
    "\n",
    "# Split the data into Train/Validation sets with stratification\n",
    "train_texts, val_texts, train_labels, val_labels, train_stratify, val_stratify = train_test_split(\n",
    "    data_texts, data_labels, stratify_values, \n",
    "    test_size=0.2, stratify=stratify_values, random_state=0\n",
    ")\n",
    "\n",
    "# Split the Train set further into Train/Test with stratification\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    train_texts, train_labels, \n",
    "    test_size=0.1, stratify=train_stratify, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View test/train/validation Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label Mapping (Encoded -> Category):\n",
      "0: Facilities Management\n",
      "1: Finance\n",
      "2: HR\n",
      "3: IT Support\n",
      "4: Marketing\n",
      "\n",
      "Final dataset information:\n",
      "Train set size: 3596\n",
      "Validation set size: 1000\n",
      "Test set size: 400\n",
      "Example train_texts: ['I’m gathering details about rewards for long-term employees and was hoping you could provide some insight. Let me know if you need further specifics from me.', 'Do you have the latest version of the diversity and inclusion policies handbook? I need it for a new hire orientation.', 'Could you share detailed insights on the performance metrics for keyword research for PPC campaigns? I’d like to use this data for our planning.']\n",
      "Example train_labels: [2, 2, 4]\n",
      "Example val_texts: ['Could you outline the steps to optimize our launching retargeting ads approach? Any case studies or examples would be helpful.', 'Need access to server maintenance.', 'Insights on customer retention strategies performance needed.']\n",
      "Example val_labels: [4, 3, 4]\n",
      "Example test_texts: ['I’m experiencing an issue with software installation. Can you assist in diagnosing and resolving the problem as soon as possible?', 'Could you review and approve the budget variance analysis for this month? Let me know if there’s anything I need to address.', 'I wanted to check on the status of my request for access to storage capacity planning. Let me know if further details are needed.']\n",
      "Example test_labels: [3, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "# Map numerical labels back to category names\n",
    "label_mapping = dict(enumerate(df['category'].astype('category').cat.categories))\n",
    "print(\"\\nLabel Mapping (Encoded -> Category):\")\n",
    "\n",
    "for encoded, category in label_mapping.items():\n",
    "    print(f\"{encoded}: {category}\")\n",
    "\n",
    "# Output dataset information\n",
    "print(\"\\nFinal dataset information:\")\n",
    "print(f\"Train set size: {len(train_texts)}\")\n",
    "print(f\"Validation set size: {len(val_texts)}\")\n",
    "print(f\"Test set size: {len(test_texts)}\")\n",
    "\n",
    "print(f\"Example train_texts: {train_texts[:3]}\") \n",
    "print(f\"Example train_labels: {train_labels[:3]}\")\n",
    "print(f\"Example val_texts: {val_texts[:3]}\") \n",
    "print(f\"Example val_labels: {val_labels[:3]}\")\n",
    "print(f\"Example test_texts: {test_texts[:3]}\") \n",
    "print(f\"Example test_labels: {test_labels[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Save test/train/val to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames for each split\n",
    "train_df = pd.DataFrame({\n",
    "    'request': train_texts,\n",
    "    'label': train_labels\n",
    "})\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    'request': val_texts,\n",
    "    'label': val_labels\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'request': test_texts,\n",
    "    'label': test_labels\n",
    "})\n",
    "\n",
    "# Save DataFrames to CSV files\n",
    "train_df.to_csv(\"data/train.csv\", index=False)\n",
    "val_df.to_csv(\"data/validation.csv\", index=False)\n",
    "test_df.to_csv(\"data/test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Data For DistilBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation = True, padding = True  )\n",
    "\n",
    "val_encodings = tokenizer(val_texts, truncation = True, padding = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    ")).batch(32)\n",
    "\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    ")).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "113/113 [==============================] - 96s 823ms/step - loss: 0.3562 - accuracy: 0.9224 - val_loss: 0.0207 - val_accuracy: 0.9980\n",
      "Epoch 2/3\n",
      "113/113 [==============================] - 94s 834ms/step - loss: 0.0139 - accuracy: 0.9986 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "113/113 [==============================] - 92s 813ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "32/32 [==============================] - 8s 258ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Validation Loss: [0.0016522674122825265, 1.0]\n"
     ]
    }
   ],
   "source": [
    "import tf_keras\n",
    "\n",
    "# Model setup\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=5)\n",
    "opt = tf_keras.optimizers.legacy.Adam(learning_rate=5e-5)\n",
    "loss = tf_keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Raw logits expected\n",
    "model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=3,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_results = model.evaluate(val_dataset)\n",
    "print(f\"Validation Loss: {evaluation_results[0]}\")\n",
    "print(f\"Validation Accuracy: {evaluation_results[1]}\")\n",
    "print(history.history.keys()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saved_models/tokenizer_config.json',\n",
       " './saved_models/special_tokens_map.json',\n",
       " './saved_models/vocab.txt',\n",
       " './saved_models/added_tokens.json')"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_directory = \"./saved_models\" \n",
    "\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
